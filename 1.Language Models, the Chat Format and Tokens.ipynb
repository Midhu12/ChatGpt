{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {
    "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630"
   },
   "source": [
    "# L1 Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {
    "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f"
   },
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cd4e96",
   "metadata": {
    "height": 149,
    "id": "19cd4e96"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key  = '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'\n",
    "\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {
    "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c"
   },
   "source": [
    "#### helper function\n",
    "Low Temperature (e.g., 0.2): The model tends to produce more focused and deterministic output. It is more likely to choose the most probable next word based on its training data.\n",
    "\n",
    "Medium Temperature (e.g., 0.5): A balance between randomness and focus. It allows for some variability in the output, making it less predictable than low temperature but not as random as high temperature.\n",
    "\n",
    "High Temperature (e.g., 0.8 or 1.0): The output becomes more creative and diverse. The model is more likely to introduce less common words and phrases, resulting in more varied and sometimes unpredictable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed96988",
   "metadata": {
    "height": 166,
    "id": "1ed96988"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a390-2461-447d-bf8b-8498db404c44",
   "metadata": {
    "id": "fe10a390-2461-447d-bf8b-8498db404c44"
   },
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cc57b2",
   "metadata": {
    "height": 47,
    "id": "e1cc57b2"
   },
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76774108",
   "metadata": {
    "height": 30,
    "id": "76774108",
    "outputId": "202778b4-6206-4481-9114-32b4650d87fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {
    "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63"
   },
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2d9e40",
   "metadata": {
    "height": 81,
    "id": "cc2d9e40",
    "outputId": "36dfdd49-735e-4564-e870-c0a53f105139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reversed letters of \"lollipop\" are \"pillipol\".\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
   "metadata": {
    "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f"
   },
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37cab84f",
   "metadata": {
    "height": 64,
    "id": "37cab84f"
   },
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1577c561",
   "metadata": {
    "height": 30,
    "id": "1577c561",
    "outputId": "71d6aec3-2be3-4be4-dae7-2f681b889b26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p-o-p-i-l-l-o-l'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {
    "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd"
   },
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f89efad",
   "metadata": {
    "height": 217,
    "id": "8f89efad"
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages,\n",
    "                                 model=\"gpt-3.5-turbo\",\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28c3424",
   "metadata": {
    "height": 200,
    "id": "b28c3424",
    "outputId": "9301c102-cf55-417b-fdbf-9446da1e5dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright and sunny,\n",
      "A happy carrot grew so funny.\n",
      "With a smile, from top to bottom,\n",
      "He was the cheeriest veggie in autumn.\n",
      "\n",
      "With a bright orange hue so bright,\n",
      "He brought joy, day and night.\n",
      "His greens were lush and oh so fine,\n",
      "A happy carrot, so divine!\n",
      "\n",
      "He danced and twirled in the breeze,\n",
      "Delighting all with his whimsical ease.\n",
      "From his leafy top to his pointy head,\n",
      "A happy carrot, nothing to dread.\n",
      "\n",
      "So let's celebrate this veggie's glee,\n",
      "A symbol of happiness, don't you see?\n",
      "In the kingdom of veggies, he's a star,\n",
      "That jolly, happy carrot, near and far!\n"
     ]
    }
   ],
   "source": [
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},\n",
    "]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c6978d",
   "metadata": {
    "height": 200,
    "id": "56c6978d",
    "outputId": "cde52c89-2fcd-4a03-f14f-2788a146c009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a cheery carrot named Charlie who lived in a vibrant garden, making everyone smile with his infectious joy.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},\n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},\n",
    "]\n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fd6331",
   "metadata": {
    "height": 234,
    "id": "14fd6331",
    "outputId": "08e33699-6b32-42e3-8417-181d19a70204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, a happy carrot grew, it danced in the sunshine and shouted, \"I love you!\"\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "]\n",
    "response = get_completion_from_messages(messages,\n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a70c79",
   "metadata": {
    "height": 387,
    "id": "89a70c79"
   },
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages,\n",
    "                                   model=\"gpt-3.5-turbo\",\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=500):\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "\n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64cf3c6",
   "metadata": {
    "height": 183,
    "id": "a64cf3c6"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\\n",
    " about a happy carrot\"\"\"},\n",
    "]\n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd8fbd4",
   "metadata": {
    "height": 30,
    "id": "cfd8fbd4",
    "outputId": "b855bd75-553d-4d49-b315-c304d89771a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, so bright and orange,\n",
      "With a smile so wide, it's hard to manage.\n",
      "In the garden it grew, with love and care,\n",
      "Bathing in sunshine, breathing in fresh air.\n",
      "\n",
      "Its roots dug deep, in the soil so fine,\n",
      "Growing tall and strong, like a vine.\n",
      "With every day that passed, it grew so sweet,\n",
      "A tasty treat, for all to eat.\n",
      "\n",
      "From the ground it was plucked, with a joyful cheer,\n",
      "For the happy carrot had nothing to fear.\n",
      "It brought smiles to faces, with its crunch and taste,\n",
      "A vegetable delight, not a moment to waste.\n",
      "\n",
      "So let's celebrate the happy carrot, so grand,\n",
      "A veggie friend, in this wondrous land.\n",
      "With its vibrant color and delightful flavor,\n",
      "It brings happiness, that we will savor.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352ad320",
   "metadata": {
    "height": 30,
    "id": "352ad320",
    "outputId": "6b50a4f1-211d-4644-fd75-32de5d016885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 36, 'completion_tokens': 168, 'total_tokens': 204}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65372cdd-d869-4768-947a-0173e7f96335",
   "metadata": {
    "id": "65372cdd-d869-4768-947a-0173e7f96335"
   },
   "source": [
    "#### Notes on using the OpenAI API outside of this classroom\n",
    "\n",
    "To install the OpenAI Python library:\n",
    "```\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f889c1-f2e4-40a5-bd27-164facb54402",
   "metadata": {
    "id": "d8f889c1-f2e4-40a5-bd27-164facb54402"
   },
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69254af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
